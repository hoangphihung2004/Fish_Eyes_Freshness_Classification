{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/manhmitcf/data.git",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:03:40.700611Z",
     "iopub.execute_input": "2025-04-26T15:03:40.701208Z",
     "iopub.status.idle": "2025-04-26T15:03:47.442664Z",
     "shell.execute_reply.started": "2025-04-26T15:03:40.701181Z",
     "shell.execute_reply": "2025-04-26T15:03:47.441733Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_name = 'ConvNeXt-Base'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:03:47.444382Z",
     "iopub.execute_input": "2025-04-26T15:03:47.444907Z",
     "iopub.status.idle": "2025-04-26T15:03:47.448553Z",
     "shell.execute_reply.started": "2025-04-26T15:03:47.444885Z",
     "shell.execute_reply": "2025-04-26T15:03:47.447808Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision import transforms\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nimport pandas as pd\nfrom PIL import Image\nfrom torch.optim.lr_scheduler import StepLR",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:03:47.449536Z",
     "iopub.execute_input": "2025-04-26T15:03:47.450110Z",
     "iopub.status.idle": "2025-04-26T15:03:56.355928Z",
     "shell.execute_reply.started": "2025-04-26T15:03:47.450091Z",
     "shell.execute_reply": "2025-04-26T15:03:56.355398Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class FishDatasetWithAugmentation(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, aug_transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.aug_transform = aug_transform\n",
    "        self.labels = {\"Highly Fresh\" : 0, \"Fresh\" : 1, \"Not Fresh\": 2}  \n",
    "        # Kiểm tra dữ liệu đầu vào\n",
    "        if not os.path.exists(img_dir) :\n",
    "            raise FileNotFoundError(f\"Thư mục ảnh '{img_dir}' không tồn tại.\")\n",
    "        if self.data.empty:\n",
    "            raise ValueError(f\"File CSV '{csv_file}' không chứa dữ liệu.\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        check = False\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 2])\n",
    "        if not os.path.exists(img_name):\n",
    "            check = True\n",
    "        if check:\n",
    "            img_name = os.path.join(self.img_dir, self.data.iloc[idx, 2])\n",
    "            img_name = img_name.replace('_5', '_#')\n",
    "            if not os.path.exists(img_name):\n",
    "                raise FileNotFoundError(f\"Không tìm thấy ảnh '{img_name}'.\")\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')  # Đọc và chuyển đổi ảnh sang RGB\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Không tìm thấy ảnh '{img_name}'.\")\n",
    "\n",
    "\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        if label not in self.labels:\n",
    "            raise ValueError(f\"Nhãn '{label}' không hợp lệ. Phải là một trong {list(self.labels.keys())}.\")\n",
    "        label = self.labels[label]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        elif self.aug_transform:\n",
    "            image = self.aug_transform(image)\n",
    "        else:\n",
    "            raise ValueError(\"Cả transform và aug_transform đều là None. Ít nhất một trong hai phải được cung cấp.\")\n",
    "\n",
    "        return image, label\n",
    "basic_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:03:56.357048Z",
     "iopub.execute_input": "2025-04-26T15:03:56.357387Z",
     "iopub.status.idle": "2025-04-26T15:03:56.367041Z",
     "shell.execute_reply.started": "2025-04-26T15:03:56.357368Z",
     "shell.execute_reply": "2025-04-26T15:03:56.366277Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import timm\nclass FishClassifier(nn.Module):\n    def __init__(self, num_classes=3):\n        super(FishClassifier, self).__init__()\n        # Load mô hình ConvNeXt-Base\n        self.convnext = timm.create_model('convnext_base', pretrained=True)\n\n        # Tính số features đầu vào của layer fully connected sau khi áp dụng GAP\n        in_features = self.convnext.num_features  # Sử dụng số lượng features đã tính toán từ mô hình ConvNeXt\n        self.convnext.head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),  # Thêm GAP layer\n            nn.Flatten(),  # Flatten output\n            nn.Linear(in_features, num_classes)  # Lớp fully connected\n        )\n\n    def forward(self, x):\n        return self.convnext(x)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:03:56.367684Z",
     "iopub.execute_input": "2025-04-26T15:03:56.367921Z",
     "iopub.status.idle": "2025-04-26T15:04:00.234356Z",
     "shell.execute_reply.started": "2025-04-26T15:03:56.367904Z",
     "shell.execute_reply": "2025-04-26T15:04:00.233739Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nimport copy\n\nclass EarlyStopping:\n    def __init__(self, patience=20, mode=\"min\"):\n        \"\"\"\n        patience: số epoch không cải thiện để dừng\n        mode: \"min\" cho val_loss (càng thấp càng tốt), \"max\" cho val_acc (càng cao càng tốt)\n        \"\"\"\n        self.patience = patience\n        self.mode = mode\n        self.best_loss = float(\"inf\")  # ban đầu giá trị loss rất lớn\n        self.best_acc = -float(\"inf\")  # ban đầu giá trị accuracy rất thấp\n        self.counter = 0\n        self.best_weights = None\n        self.early_stop = False\n\n    def check_improvement(self, val_loss, val_acc, model):\n        \"\"\"\n        Kiểm tra cải thiện dựa trên cả val_loss và val_acc.\n        \"\"\"\n        # Kiểm tra có cải thiện `val_loss` hoặc `val_acc`\n        if val_acc > self.best_acc:\n            # Nếu `val_loss` giảm hoặc `val_acc` tăng, cập nhật mô hình tốt nhất\n            self.best_loss = val_loss\n            self.best_acc = val_acc\n            self.best_weights = copy.deepcopy(model.state_dict())  # Lưu lại trọng số của mô hình\n            self.counter = 0  # Reset counter vì đã có cải thiện\n            return True\n\n        # Nếu không có cải thiện\n        self.counter += 1\n        if self.counter >= self.patience:\n            self.early_stop = True  # Nếu không có cải thiện sau `patience` epoch thì dừng\n        return False\n\n    def restore_best_model(self, model):\n        \"\"\"Khôi phục lại mô hình tốt nhất\"\"\"\n        if self.best_weights is not None:\n            model.load_state_dict(self.best_weights)\n            print(\"Restored best model weights.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:05:06.000797Z",
     "iopub.execute_input": "2025-04-26T15:05:06.001112Z",
     "iopub.status.idle": "2025-04-26T15:05:06.007732Z",
     "shell.execute_reply.started": "2025-04-26T15:05:06.001083Z",
     "shell.execute_reply": "2025-04-26T15:05:06.007027Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "# Cấu hình\n",
    "TRAIN_CSV_PATH = \"data/train.csv\"\n",
    "VAL_CSV_PATH = \"data/val.csv\"\n",
    "IMG_DIR = \"data/images/\"\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 3\n",
    "early_stopper = EarlyStopping(patience=20, mode=\"min\")  # Hoặc \"max\" nếu bạn muốn theo dõi accuracy\n",
    "epoch = None \n",
    "# Dataset và DataLoader\n",
    "train_dataset = FishDatasetWithAugmentation(\n",
    "    csv_file=TRAIN_CSV_PATH,\n",
    "    img_dir=IMG_DIR,\n",
    "    transform=None,\n",
    "    aug_transform=aug_transform,  \n",
    ")\n",
    "\n",
    "val_dataset = FishDatasetWithAugmentation(\n",
    "    csv_file=VAL_CSV_PATH,\n",
    "    img_dir=IMG_DIR,\n",
    "    transform=basic_transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model + Loss + Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FishClassifier(num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "# Log để lưu lại loss/acc\n",
    "loss_train, loss_val = [], []\n",
    "acc_train, acc_val = [], []\n",
    "total_start_time = time.time()\n",
    "epoch_times = []\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time() \n",
    "    # Train\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS} (Train)\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_running_loss / len(train_dataloader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    # Lưu log\n",
    "    loss_train.append(avg_train_loss)\n",
    "    loss_val.append(avg_val_loss)\n",
    "    acc_train.append(train_accuracy)\n",
    "    acc_val.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"    Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"    Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    # Sau khi tính avg_val_loss và val_accuracy\n",
    "    if early_stopper.check_improvement(avg_val_loss, val_accuracy, model):\n",
    "        print(\"Improved! Saving best model.\")\n",
    "    else:\n",
    "        print(f\"No improvement for {early_stopper.counter} epochs.\")\n",
    "\n",
    "    # Nếu không cải thiện trong `patience` epochs, dừng sớm và khôi phục mô hình tốt nhất\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        early_stopper.restore_best_model(model)\n",
    "        break\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    epoch_times.append(epoch_duration)  # Lưu thời gian epoch này\n",
    "# Tính thời gian kết thúc toàn bộ training\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal training time: {total_duration/60:.2f} minutes.\")\n",
    "# Trung bình thời gian 1 epoch\n",
    "avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "print(f\"Average time per epoch: {avg_epoch_time:.2f} seconds (~{avg_epoch_time/60:.2f} minutes)\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T15:05:07.652881Z",
     "iopub.execute_input": "2025-04-26T15:05:07.653173Z",
     "iopub.status.idle": "2025-04-26T17:21:43.292433Z",
     "shell.execute_reply.started": "2025-04-26T15:05:07.653130Z",
     "shell.execute_reply": "2025-04-26T17:21:43.291629Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport os\n\n# Đảm bảo thư mục 'result' tồn tại\nos.makedirs('result', exist_ok=True)\n\n# Vẽ Loss\nplt.figure()\nplt.plot(loss_train, label='Train Loss')\nplt.plot(loss_val, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\nplt.grid(True)\nplt.savefig('result/loss_curve.png')  # Lưu vào file\nplt.close()\n\n# Vẽ Accuracy\nplt.figure()\nplt.plot(acc_train, label='Train Accuracy')\nplt.plot(acc_val, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Epochs')\nplt.legend()\nplt.grid(True)\nplt.savefig('result/accuracy_curve.png')  # Lưu vào file\nplt.close()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:43.293759Z",
     "iopub.execute_input": "2025-04-26T17:21:43.294275Z",
     "iopub.status.idle": "2025-04-26T17:21:43.600024Z",
     "shell.execute_reply.started": "2025-04-26T17:21:43.294255Z",
     "shell.execute_reply": "2025-04-26T17:21:43.599325Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\ntorch.save(model.state_dict(), f\"result/fish_classifier_{model_name}.pth\")\nprint(\"Đã lưu mô hình!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:43.600815Z",
     "iopub.execute_input": "2025-04-26T17:21:43.601066Z",
     "iopub.status.idle": "2025-04-26T17:21:44.035740Z",
     "shell.execute_reply.started": "2025-04-26T17:21:43.601044Z",
     "shell.execute_reply": "2025-04-26T17:21:44.035098Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport timm\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score,\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\nimport matplotlib.pyplot as plt",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:44.037441Z",
     "iopub.execute_input": "2025-04-26T17:21:44.038132Z",
     "iopub.status.idle": "2025-04-26T17:21:44.041854Z",
     "shell.execute_reply.started": "2025-04-26T17:21:44.038104Z",
     "shell.execute_reply": "2025-04-26T17:21:44.041318Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# ==== Load dataset test ====\n",
    "CSV_PATH = \"data/test.csv\"\n",
    "IMG_DIR = \"data/images/\"\n",
    "\n",
    "try:\n",
    "    dataset = FishDatasetWithAugmentation(CSV_PATH, IMG_DIR, transform=basic_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Lỗi khi tải dataset: {e}\")\n",
    "\n",
    "# ==== Dự đoán và tính metrics ====\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "\n",
    "# ==== Classification Report + Confusion Matrix ====\n",
    "try:\n",
    "    class_names = [\"Highly Fresh\", \"Fresh\", \"Not Fresh\"]\n",
    "    if len(set(all_labels)) > len(class_names):\n",
    "        raise ValueError(\"Số lượng lớp thực tế lớn hơn số lớp được định nghĩa.\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    # Vẽ và lưu\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Bạn có thể chỉnh kích thước tùy ý\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig('result/confusion_matrix.png', dpi=300)  # Lưu vào file PNG\n",
    "    plt.close()  # Đóng plot để không bị chồng hình khi vẽ tiếp\n",
    "\n",
    "    print(\"Đã lưu confusion matrix vào thư mục 'result/'.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Lỗi trong việc tạo báo cáo lớp: {e}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:44.042482Z",
     "iopub.execute_input": "2025-04-26T17:21:44.042671Z",
     "iopub.status.idle": "2025-04-26T17:21:54.708088Z",
     "shell.execute_reply.started": "2025-04-26T17:21:44.042650Z",
     "shell.execute_reply": "2025-04-26T17:21:54.707476Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "optimize = \"AdamW\"",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.708862Z",
     "iopub.execute_input": "2025-04-26T17:21:54.709064Z",
     "iopub.status.idle": "2025-04-26T17:21:54.712759Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.709048Z",
     "shell.execute_reply": "2025-04-26T17:21:54.711979Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "results = {\n    \"model_name\": model_name,\n    \"optimizer\": optimize,\n    \"lr\": LEARNING_RATE,\n    \"batch_size\": BATCH_SIZE,\n    \"epochs\": EPOCHS,\n    \"epochs_current\": epoch,\n    \"val_best_acc\": early_stopper.best_acc * 0.01,\n    \"val_best_loss\": early_stopper.best_loss,\n    \"accuracy\": acc,\n    \"precision\": precision,\n    \"recall\": recall,\n    \"f1_score\": f1,\n    \"time(s)\": total_duration,\n    \"time_per_epoch(s)\": avg_epoch_time\n}",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.713606Z",
     "iopub.execute_input": "2025-04-26T17:21:54.713854Z",
     "iopub.status.idle": "2025-04-26T17:21:54.729683Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.713831Z",
     "shell.execute_reply": "2025-04-26T17:21:54.729098Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "results_df = pd.DataFrame([results])\nresults_df.to_csv(f\"result/evaluation_results_{model_name}.csv\", index=False)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.730261Z",
     "iopub.execute_input": "2025-04-26T17:21:54.730458Z",
     "iopub.status.idle": "2025-04-26T17:21:54.753640Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.730443Z",
     "shell.execute_reply": "2025-04-26T17:21:54.752957Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_config(save_dir,config):\n",
    "\n",
    "    # Xóa các trường None để file json gọn\n",
    "    config = {k: v for k, v in config.items() if v is not None}\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}_config.json\")\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Đã lưu file config: {save_path}\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.754611Z",
     "iopub.execute_input": "2025-04-26T17:21:54.754815Z",
     "iopub.status.idle": "2025-04-26T17:21:54.760088Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.754800Z",
     "shell.execute_reply": "2025-04-26T17:21:54.759339Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "config = {\n    \"model_name\": model_name,\n    \"input_size\": 224,\n    \"num_classes\": 3,\n    \"batch_size\": BATCH_SIZE,\n    \"learning_rate\": LEARNING_RATE,\n    \"optimizer\": optimize,\n    \"loss_function\": \"CrossEntropyLoss\",\n    \"weight_decay\": 0.02,\n    \"epoch_trained\": epoch,\n    \"best_val_acc\": early_stopper.best_acc * 0.01,\n    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"loss_train\": loss_train,\n    \"loss_val\": loss_val,\n    \"acc_train\": acc_train,\n    \"acc_val\": acc_val\n}",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.762070Z",
     "iopub.execute_input": "2025-04-26T17:21:54.762520Z",
     "iopub.status.idle": "2025-04-26T17:21:54.779884Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.762494Z",
     "shell.execute_reply": "2025-04-26T17:21:54.779091Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_model_config(save_dir=\"result\",config = config)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.780707Z",
     "iopub.execute_input": "2025-04-26T17:21:54.780947Z",
     "iopub.status.idle": "2025-04-26T17:21:54.794286Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.780925Z",
     "shell.execute_reply": "2025-04-26T17:21:54.793734Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import FileLink\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Giả sử model_name đã được định nghĩa\n",
    "zip_filename = f\"result_{model_name}.zip\"\n",
    "\n",
    "# Nén folder\n",
    "shutil.make_archive(base_name=zip_filename.replace('.zip', ''), format='zip', root_dir='result')\n",
    "\n",
    "# Tạo link tải\n",
    "print(\"File đã sẵn sàng để tải:\")\n",
    "display(FileLink(zip_filename))\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-26T17:21:54.794994Z",
     "iopub.execute_input": "2025-04-26T17:21:54.795542Z",
     "iopub.status.idle": "2025-04-26T17:22:12.110858Z",
     "shell.execute_reply.started": "2025-04-26T17:21:54.795522Z",
     "shell.execute_reply": "2025-04-26T17:22:12.110036Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
